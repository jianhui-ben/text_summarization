{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>562971</td>\n",
       "      <td>I love these cookies!  Not only are they healt...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>562972</td>\n",
       "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>562973</td>\n",
       "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>562974</td>\n",
       "      <td>I participated in a product review that includ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>562975</td>\n",
       "      <td>My kids loved these. I was very pleased to giv...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                               Text  Score\n",
       "0  562971  I love these cookies!  Not only are they healt...      5\n",
       "1  562972  Quaker Soft Baked Oatmeal Cookies with raisins...      5\n",
       "2  562973  I am usually not a huge fan of oatmeal cookies...      5\n",
       "3  562974  I participated in a product review that includ...      5\n",
       "4  562975  My kids loved these. I was very pleased to giv...      5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"Amazon_review_data/Reviews.csv\")\n",
    "reviews['ProductId']=reviews['ProductId'].astype(str)\n",
    "df=reviews[reviews['ProductId']==\"B007JFMH8M\"]  ## the most popular cookie\n",
    "df = df.reset_index(drop=True)\n",
    "df= df[['Id', 'Text', 'Score']]\n",
    "sentences=df.Text.values\n",
    "labels= df.Score.values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the bert model\n",
    "model = BertModel.from_pretrained('bert-base-uncased') #cache_dir='some path that I want to download the model'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) #output_hidden_states=True, output_attentions=True\n",
    "# input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([804.,  90.,  10.,   5.,   2.,   1.,   0.,   0.,   0.,   1.]),\n",
       " array([  21. ,  122.6,  224.2,  325.8,  427.4,  529. ,  630.6,  732.2,\n",
       "         833.8,  935.4, 1037. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens_counts=[len(tokenizer.tokenize(sent)) for sent in sentences]\n",
    "print(sum(np.array(tokens_counts)>256))\n",
    "plt.hist(tokens_counts)  ## most texts is shorted than 400 tokens, we select 256 as the max_len of our sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  I love these cookies!  Not only are they healthy but they taste great and are so soft!  I will definitely add these to my grocery list!\n",
      "Tokenized:  ['i', 'love', 'these', 'cookies', '!', 'not', 'only', 'are', 'they', 'healthy', 'but', 'they', 'taste', 'great', 'and', 'are', 'so', 'soft', '!', 'i', 'will', 'definitely', 'add', 'these', 'to', 'my', 'grocery', 'list', '!']\n",
      "Token IDs:  [1045, 2293, 2122, 16324, 999, 2025, 2069, 2024, 2027, 7965, 2021, 2027, 5510, 2307, 1998, 2024, 2061, 3730, 999, 1045, 2097, 5791, 5587, 2122, 2000, 2026, 13025, 2862, 999]\n",
      "Token IDs aftern encoding:  [101, 1045, 2293, 2122, 16324, 999, 2025, 2069, 2024, 2027, 7965, 2021, 2027, 5510, 2307, 1998, 2024, 2061, 3730, 999, 1045, 2097, 5791, 5587, 2122, 2000, 2026, 13025, 2862, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "input_token_ids=[]\n",
    "for sent in sentences:\n",
    "    input_token_ids.append(tokenizer.encode(sent,                         \n",
    "                            max_length = 256))      # Truncate all sentences.\n",
    "#                             return_tensors = 'pt'))     # Return pytorch tensors.\n",
    "\n",
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "## tokenize.encode can handle both two steps above, and then add special tokens [start] and [end]\n",
    "print('Token IDs aftern encoding: ', tokenizer.encode(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  256\n",
      "Min sentence length:  23\n"
     ]
    }
   ],
   "source": [
    "## max length of the reviews:\n",
    "print('Max sentence length: ', max([len(sen) for sen in input_token_ids]))\n",
    "print('Min sentence length: ', min([len(sen) for sen in input_token_ids]))  ##padding has not been added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 256 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Padding added done\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids=[]\n",
    "for sent in input_token_ids:\n",
    "    input_ids.append(F.pad(torch.Tensor(sent), (int((MAX_LEN-len(sent))/2)+(MAX_LEN-len(sent))%2, \n",
    "                                                int((MAX_LEN-len(sent))/2)), \"constant\", 0))\n",
    "print('\\nPadding added done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(torch.Tensor(np.array(att_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=110, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=110, test_size=0.1)\n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.stack(train_inputs)  ##convert a list of tensors into a tensor\n",
    "validation_inputs = torch.stack(validation_inputs)\n",
    "\n",
    "train_labels = torch.Tensor(train_labels) ## convert an array to a tensor\n",
    "validation_labels = torch.Tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.stack(train_masks)\n",
    "validation_masks = torch.stack(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertForSequenceClassification.from_pretrained??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 5, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (5, 768)\n",
      "classifier.bias                                                 (5,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f00201a08d0>]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the purposes of fine-tuning, the authors recommend choosing from the following values:\n",
    "# - Batch size: 16, 32  (We chose 32 when creating our DataLoaders).\n",
    "# - Learning rate (Adam): 5e-5, 3e-5, 2e-5  (We'll use 2e-5).\n",
    "# - Number of epochs: 2, 3, 4  (We'll use 4).\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "#'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps =1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "##plot the lrs\n",
    "lrs=[]\n",
    "for i in range(100):\n",
    "    lrs.append(scheduler.get_lr()) \n",
    "    scheduler.step()\n",
    "plt.plot(range(100), lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.0001]\n",
      "1 [0.0001]\n",
      "2 [0.0001]\n",
      "3 [0.0001]\n",
      "4 [0.0001]\n",
      "5 [0.0001]\n",
      "6 [0.0001]\n",
      "7 [0.0001]\n",
      "8 [0.0001]\n",
      "9 [0.0001]\n"
     ]
    }
   ],
   "source": [
    "test = torch.autograd.Variable(torch.randn([5,5]), requires_grad=True)\n",
    "optimizer = torch.optim.Adam([test], lr = 0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, lr_scheduler.get_lr())\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##next part: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.65625"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "821/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([821, 256])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([821, 256])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.stack(train_inputs)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-08e540e74da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "validation_inputs = torch.tensor(validation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101.,  1045.,  2293.,  2122., 16324.,   999.,  2025.,  2069.,  2024.,\n",
       "         2027.,  7965.,  2021.,  2027.,  5510.,  2307.,  1998.,  2024.,  2061.,\n",
       "         3730.,   999.,  1045.,  2097.,  5791.,  5587.,  2122.,  2000.,  2026.,\n",
       "        13025.,  2862.,   999.,   102.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(input_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 31])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=input_token_ids[0]\n",
    "F.pad(test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((256-31)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "t4d = torch.rand([1,31])\n",
    "p1d = (int((256-31)/2)+1,int((256-31)/2)) # pad last dim by 1 on each side\n",
    "out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
    "print(out.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face_ENV",
   "language": "python",
   "name": "hugging_face_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
